# -*- coding: utf-8 -*-
"""jetson_transfer_learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WWSAVECGX-E8pjRVdz_Bi0WiY0_jGBvx
"""

# Unzipping the data
!unzip -q dataset.zip

# Importing the needed packages
import numpy as np
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.inception_v3 import InceptionV3,preprocess_input,decode_predictions
from tensorflow.keras.preprocessing import image
from tensorflow.keras.callbacks import EarlyStopping

# Specifying the parameters of the input images, with respect to the Inception V3 specs
img_height=299
img_width=299

# Constructing the model
base_model = InceptionV3(weights='imagenet', include_top=False) # Our base model is the Inception V3 without the final FC layers
x = base_model.output

# After the convnet we place a flatten and two dense layers in order to create a classification of our own
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(1, activation='sigmoid')(x) # We use one neuron because our task is a binary classification task
model = Model(inputs=base_model.input, outputs=predictions)

# We freeze the layers of the base model, because we only want to train pur classifier.
for layer in base_model.layers:
    layer.trainable = False

# Compiling the model
model.compile(optimizer='adam', metrics=['accuracy'],loss='binary_crossentropy')

# Data normalization and augmentation
train_datagen = ImageDataGenerator(rescale=1./255, brightness_range=(0.5,0.9), zoom_range=0.2, horizontal_flip=True, validation_split=0.2)
valid_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)
train_data = train_datagen.flow_from_directory("dataset", target_size=(img_height,img_width), class_mode="binary", batch_size=20, subset="training")
valid_data = valid_datagen.flow_from_directory("dataset", target_size=(img_height,img_width), class_mode="binary", batch_size=20, subset="validation")

# Regularization using eraly stopping
callback = EarlyStopping(monitor="val_loss", patience=2, restore_best_weights=True)

# Training the network
history = model.fit(x=train_data, epochs=10, verbose=2, validation_data=valid_data, callbacks=callback)

# Visualizing the performance of the training

import matplotlib.pyplot as plt
plt.style.use('ggplot')

def plot_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training mse')
    plt.plot(x, val_acc, 'r', label='Validation accuracy')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()


loss, accuracy = model.evaluate(train_data, verbose=False)
print("Training Loss: {:.4f}".format(loss))
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model.evaluate(test_data, verbose=False)
print("Validation Loss:  {:.4f}".format(loss))
print("Validation Accuracy:  {:.4f}".format(accuracy))
plot_history(history)

# Saving the trained model
model.save("/content")